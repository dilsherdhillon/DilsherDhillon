---
title: "Multiple Comparisons and Bayesian Inference"  
date: "2020-02-02"  
output:
  blogdown::html_page:
    toc: true     
---


<div id="TOC">
<ul>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#background">Background</a></li>
<li><a href="#bayesian-inference-and-anova">Bayesian Inference and ANOVA</a></li>
<li><a href="#simulation-set-up">Simulation Set up</a><ul>
<li><a href="#frequentist-pairwise-comparisons">Frequentist pairwise comparisons</a><ul>
<li><a href="#naive">Naive</a></li>
<li><a href="#tukey-adjusted">Tukey adjusted</a></li>
</ul></li>
<li><a href="#multilevel-model">Multilevel Model</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
</ul>
</div>

<div id="motivation" class="section level1">
<h1>Motivation</h1>
<p>They say the best way to learn something, is to teach it! And that’s exactly what I intend to do. Inspired by <a href="https://solomonkurz.netlify.com/post/bayesian-power-analysis-part-i/">Solomon Kurz’s</a> blog posts on power calcualtions in bayesian inference, and Dr. Gelman’s blogs, here’s an attempt to open the guts of a problem I’ve struggled with.</p>
<p><strong>I don’t have a whole lot of experience in simulations, so looking forward hearing how I can improve my assumptions to reach a valid conclusion.</strong></p>
</div>
<div id="background" class="section level1">
<h1>Background</h1>
<p>As an applied statistician, trained primarily under a frequentist paradigm, I’ve been taught and have used frequentist inference for all the work I’ve done - up until sometime last year. Except for that one bayesian class in school, which focused on laying the probability distribution ground work, I really was never exposed to tools to use bayesian inference in an applied setting. Although I thank the 2017 me for taking that elective, since it’s helped me better understand the tools available for bayesian inference in a applied setting(looking at you <a href="https://mc-stan.org/">Stan</a> and <a href="https://github.com/paul-buerkner/brms">brms</a>.</p>
</div>
<div id="bayesian-inference-and-anova" class="section level1">
<h1>Bayesian Inference and ANOVA</h1>
<p>I work in an industry where there is a heavy emphasis on design of experiments(DOE). Since experimentation can sometimes be expensive, and there are factors that are hard to change, analyzing these experiments is almost never a simple ANOVA or ANCOVA - it almost always requires you to get the structure of the problem correct.
For example, a very common error is not being able to spot a <a href="https://newonlinecourses.science.psu.edu/stat503/node/71/">split plot</a> design. Where one of the factors is randomized first, and then the second factor is applied “on” the first.</p>
<p>Another is the <em>multiple comparison</em> problem, the theme of this post, where more the number of post-hoc comparisons, the more likely we are to reject the null and make a type I error. That’s when you have the bonferroni’s, the tukey’s, the dunnett’s to control for the family wise error rate. There’s extensive work done in this, and Tukey is well accepted post-hoc correction.</p>
<p><strong>But what about multiple comparisons in bayesian inference?</strong></p>
<p>Does a bayesian worry about multiple testing? Even though making a type I error is associated with rejecting the null, when in fact the null is true, which is a frequentist paradigm - a bayesian may still be concerned about an uncertainity interval excluding 0, when in fact, 0 is a plausible value in the posterior distribution.</p>
<p>And so a quick google-fu later, of course, Dr. Gelman has written a lot about this topic. <a href="https://statmodeling.stat.columbia.edu/2016/08/22/bayesian-inference-completely-solves-the-multiple-comparisons-problem/">This</a> blog post gives a nice intuitive explanation. Also, if you have a copy of his book BDA 3rd edition, page 96 goes over this too.</p>
<p>The gist is, if you’re putting a prior on your paramters, Dr. Gelman shows you’re less likely to conclude there is an effect when infact there’s none. So if you examine your a priori beliefs and put regularizing priors on your parameters, your estimates tend to be <em>skeptical</em> of the data, and hence, you’re less likely to conclude that 0 is not a plausible value of the posterior distribution of the parameters.</p>
<p>But what if, for example, like a recent study I analyzed, there’s a two-factorial design, with 6 levels to each factor, AND there’s an interaction going on so we’d like to test for differences in the 6 levels of one factor, at EVERY level of the factor. That’s 90 pairwise comparisons. It’s quite realistic to assume that you can’t put an informative prior on every single interaction estimate. What does one do in this situation? Well, the answer, if you haven’t already guessed it, are <strong>multilevel</strong> models. <a href="http://www.stat.columbia.edu/~gelman/research/published/AOS259.pdf">Here’s</a> the paper Dr. Gelman linked me to when I asked him about it. He argues every analysis should be run as a multilevel model.</p>
<p>This post is too short to explain in detail what multilevel models are but if you’ve ever taken a class or looked through a tutorial of mixed effect models, where one of the factors is a considered a “random effect”, well in this case, <strong>every</strong> factor is a random effect.</p>
<p>Here are a few of the many resources available that would help get a sense of them.<br />
<a href="https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html">partial pooling using heirarchical models</a><br />
<a href="https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/">Plotting partial pooling models using lme4</a></p>
<p>We’ll see below that in fact, a running a multilevel model, you’re less likely as compared with a fixed effect ANOVA model without controlling multiple comparisons, but also as compared to comparing with Tukey controlled estimates.</p>
<p>In addition, we’ll build the multilevel models using the bayesian inference engine Stan.<br />
<strong>Note</strong> - Multilevel models can also be constructed using frequentist methods - library <code>lmer</code> will do this but as Gelman points out in Chapter 5 of BDA, there’s challenges with this especially when we’re working with small sample sizes (negative standard deviations). Whereas under a bayesian framework, you place priors on the standard deviations that restrict them to realistic values.</p>
</div>
<div id="simulation-set-up" class="section level1">
<h1>Simulation Set up</h1>
<p>We’ll compare three simulations.<br />
1. A one - way ANOVA with 5 levels of a factor, conducting 10 pairwise comparisons, without controlling the Type 1 error<br />
2. A one - way ANOVA with 5 levels of a factor, conducting 10 pairwise comparisons, controlling for Type 1 error using the Tukey correction<br />
3. A one - way ANOVA with 5 levels of a factor, but estimated using a multilevel model, pooling estimates from other groups, under a bayesian framework.</p>
<p>We’ll be using the <a href="https://cran.r-project.org/web/packages/emmeans/vignettes/interactions.html">emmeans</a> for frequentist comparisons, and the <a href="https://github.com/paul-buerkner/brms">brms</a> for the bayesian multilevel models.</p>
<div id="frequentist-pairwise-comparisons" class="section level2">
<h2>Frequentist pairwise comparisons</h2>
<p>Let’s set up a simple linear model with 5 groups, and each group has 5 replications each.</p>
<pre class="r"><code>library(tidyverse)
library(brms)
library(emmeans)</code></pre>
<pre class="r"><code>## these are the names of all the dogs I&#39;ve had :) 
treatmentA &lt;-rep(c(&quot;snoopy&quot;,&quot;ginger&quot;,&quot;sugar&quot;,&quot;spicy&quot;,&quot;lisa&quot;), times = 3)

d &lt;- data.frame(treatmentA = treatmentA, 
            response = rnorm(15, 10, 20))

## Initialize a model so we can simply update simulated data
fit &lt;- lm(response ~ treatmentA, data = d)

## This is where all the magic takes place  
sim_aov &lt;- function(seed) {
  set.seed(seed)
  ## simulate new data  
  d &lt;- data.frame(treatmentA = treatmentA, 
            response = rnorm(15, 10, 20))
  ## Update model with new data  
  fit &lt;- update(fit, data = d)
  
  ## Calculate pairwise comparisons naive p-values using the emmeans package  
  n_naive &lt;-
    emmeans::emmeans(fit, pairwise ~ treatmentA, adjust = &quot;none&quot;) %&gt;%
    purrr::pluck(2) %&gt;%
    dplyr::tbl_df() %&gt;%
    dplyr::filter(p.value &lt; 0.05) %&gt;%
    nrow(.)
  
  ## Calculate pairwise comparisons tukey adjusted p-values using the emmeans package  
  n_tukey &lt;-
    emmeans::emmeans(fit, pairwise ~ treatmentA, adjust = &quot;tukey&quot;) %&gt;%
    purrr::pluck(2) %&gt;%
    dplyr::tbl_df() %&gt;%
    dplyr::filter(p.value &lt; 0.05) %&gt;%
    nrow(.)
  
  output &lt;- list(n_naive, n_tukey)  
}</code></pre>
<p>Now we’re ready to simulate the above models!</p>
<pre class="r"><code>seeds &lt;- rep(1:1000)
sim_results &lt;- purrr::map(seeds, function(x)
  sim_aov(seed = x))</code></pre>
<p>We’ll separate out the naive p-vaues and the tukey adjusted p values. I’ve used the 0.05 threshold to call out if something is significant or not.</p>
<pre class="r"><code>naive_p &lt;- sapply(sim_results,`[[`,1) 
tukey_p &lt;- sapply(sim_results,`[[`,2) </code></pre>
<p>There’s two ways to look at it -</p>
<ol style="list-style-type: decimal">
<li><p>Of the 1000 models run, how many TOTAL pairwise comparisons resulted in p&lt;0.05. This corresponds to 10 tests in each model, so a total of 10,000 tests.</p></li>
<li><p>On the other hand, of the 1000 models, how many had AT LEAST one signficant result? That corresponds to a 1000 tests.</p></li>
</ol>
<div id="naive" class="section level3">
<h3>Naive</h3>
<p>Total number of significant pairwise comparisons?</p>
<pre class="r"><code>s &lt;- sum(naive_p)
s</code></pre>
<pre><code>## [1] 527</code></pre>
<p>How many times did we see atleast one pairwise comparison significant?</p>
<pre class="r"><code>t &lt;- (1000 - sum(naive_p == 0))
t</code></pre>
<pre><code>## [1] 255</code></pre>
<p>That means, 25.5 % of the time, we see atleast one pairwise comparison significant, when in fact, there is no difference.</p>
<p>That’s not surprising, we expect the error rates to be that high. And thats’ why we have the corrections we normally apply.</p>
</div>
<div id="tukey-adjusted" class="section level3">
<h3>Tukey adjusted</h3>
<p>Total number of significnat pairwise comparisons?</p>
<pre class="r"><code>sum(tukey_p)</code></pre>
<pre><code>## [1] 77</code></pre>
<p>Dropped down to an error rate of 0.0077</p>
<p>How many times did we see atleast one pairwise comparison significant?</p>
<pre class="r"><code>(1000 - sum(tukey_p == 0))</code></pre>
<pre><code>## [1] 58</code></pre>
<p>Dropped down to an error rate of 0.058</p>
<p>Seems like Tukey correction is actually doing it’s job, which is good news.</p>
<p>Let’s move on to the multilevel models now!</p>
</div>
</div>
<div id="multilevel-model" class="section level2">
<h2>Multilevel Model</h2>
<p>Let’s first run a one-model example and we’ll write a function for it to then iterate over simualted data sets. Let’s introduce <a href="https://github.com/paul-buerkner/brms">Paul Buerkner’s</a> <code>brms</code> library.</p>
<p>Instead of estimating the LS means, we’ll estimating separate intercepts for each level in the factor. The syntax is very similar to <code>lme4</code>. For more information on brms and it’s documentation, <a href="https://github.com/paul-buerkner/brms">check out the github rep</a></p>
<pre class="r"><code>library(brms)
d_bayes &lt;- data.frame(treatmentA = as_factor(rep(c(1,2,3,4,5), each = 3)), response = rnorm(15,0,2))
## initialize the model  
mod_brms &lt;- brms::brm(response ~  (1|treatmentA), data = d_bayes, chains = 4, cores = 4, 
                      prior = c(
    prior(normal(0, 2), &quot;Intercept&quot;),
    prior(student_t(3, 0, 1), &quot;sd&quot;),
    prior(student_t(3, 0, 2), &quot;sigma&quot;)), family = gaussian)
## We&#39;ll be using weakly informative priors 
saveRDS(mod_brms,&quot;/Users/dilsherdhillon/Documents/DilsherDhillon/static/files/mod_brms.rds&quot;)</code></pre>
<p>NB -
The same model can be run using the <code>rethinking</code> library by Dr. McElreath. Since I’m currently going through his lectures and homeworks, I’d be remiss if I didn’t show you how to do it using <code>rethinking</code></p>
<pre class="r"><code>data_list &lt;- list(
  treatmentA = as.integer(d_bayes$treatmentA),
  response = scale(d_bayes$response)
)

f &lt;- alist(
  response ~ dnorm(mu, sigma),
  mu &lt;- a_trt[treatmentA] ,
  ## adaptive priors  
  a_trt[treatmentA] ~ dnorm(a_bar, sigma_A),
  ## hyper priors  
  a_bar ~ dnorm(0,1),
  sigma ~ dexp(1),
  sigma_A ~ dexp(1)
 )
mod &lt;- rethinking::ulam(
  f, data = data_list, chains = 4, iter = 2000, control=list(adapt_delta=0.95), cores = 4
)
## You&#39;ll notice I used different priors, but it really doesn&#39;t matter in the context of this problem.   
## THIS CODE NOT RUN  </code></pre>
<p>Pairwise comparisons aren’t as straightforward as a linear model we ran above. We’ll first need to extract out the estimates from the posterior, and compute these pairwise differences and their 95% intervals by hand. It’s more fun this way!</p>
<pre class="r"><code>mod_brms &lt;- readRDS(&quot;/Users/dilsherdhillon/Documents/DilsherDhillon/static/files/mod_brms.rds&quot;)
## extract the intercepts  
  post &lt;- brms::posterior_samples(mod_brms) %&gt;% 
    select(contains(&quot;r_treatmentA&quot;))
str(post) </code></pre>
<pre><code>## &#39;data.frame&#39;:    4000 obs. of  5 variables:
##  $ r_treatmentA[1,Intercept]: num  9.72e-05 -1.30e-03 -2.47e-03 -8.70e-01 -3.49e-01 ...
##  $ r_treatmentA[2,Intercept]: num  -0.001619 -0.000941 -0.002774 0.13095 -0.036746 ...
##  $ r_treatmentA[3,Intercept]: num  0.00323 0.00545 0.00535 -0.73711 -0.3627 ...
##  $ r_treatmentA[4,Intercept]: num  -0.000601 -0.000492 -0.0003 -1.332787 -0.877247 ...
##  $ r_treatmentA[5,Intercept]: num  0.000691 0.002601 0.002058 2.036914 1.383762 ...</code></pre>
<p>We’ll make use of the helpful <code>outer</code> function and some more wrangling to get it in the right format.</p>
<pre class="r"><code>pairs &lt;- outer(colnames(post), colnames(post), paste, sep = &quot;-&quot;)
index &lt;-  which(lower.tri(pairs, diag = TRUE))
comparisons &lt;- outer(1:ncol(post), 1:ncol(post),
                     function(x, y)
                       post[, x] - post[, y])
colnames(comparisons) &lt;- pairs
comparisons &lt;- comparisons[-index]

str(comparisons)  </code></pre>
<pre><code>## &#39;data.frame&#39;:    4000 obs. of  10 variables:
##  $ r_treatmentA[1,Intercept]-r_treatmentA[2,Intercept]: num  0.00172 -0.00036 0.0003 -1.0013 -0.31268 ...
##  $ r_treatmentA[1,Intercept]-r_treatmentA[3,Intercept]: num  -0.00313 -0.00675 -0.00783 -0.13324 0.01328 ...
##  $ r_treatmentA[2,Intercept]-r_treatmentA[3,Intercept]: num  -0.00485 -0.00639 -0.00813 0.86806 0.32596 ...
##  $ r_treatmentA[1,Intercept]-r_treatmentA[4,Intercept]: num  0.000698 -0.000809 -0.002174 0.462436 0.527824 ...
##  $ r_treatmentA[2,Intercept]-r_treatmentA[4,Intercept]: num  -0.001017 -0.000449 -0.002474 1.463736 0.840501 ...
##  $ r_treatmentA[3,Intercept]-r_treatmentA[4,Intercept]: num  0.00383 0.00594 0.00565 0.59568 0.51454 ...
##  $ r_treatmentA[1,Intercept]-r_treatmentA[5,Intercept]: num  -0.000593 -0.003902 -0.004532 -2.907265 -1.733185 ...
##  $ r_treatmentA[2,Intercept]-r_treatmentA[5,Intercept]: num  -0.00231 -0.00354 -0.00483 -1.90596 -1.42051 ...
##  $ r_treatmentA[3,Intercept]-r_treatmentA[5,Intercept]: num  0.00254 0.00285 0.00329 -2.77402 -1.74647 ...
##  $ r_treatmentA[4,Intercept]-r_treatmentA[5,Intercept]: num  -0.00129 -0.00309 -0.00236 -3.3697 -2.26101 ...</code></pre>
<p>Now we want all the comparisons, that excluded zero, in either direction (Dr. Gelman calls this the Type S error, but that’s outside the scope of this post)</p>
<pre class="r"><code>  ## this gets us the number of comparisons that excluded zero  
  comparisons %&gt;%
    gather() %&gt;%
    group_by(key) %&gt;%
    mutate(mean = mean(value), lower_ci = quantile(value, 0.025), upper_ci = quantile(value, 0.975)) %&gt;%
    distinct(mean, .keep_all = TRUE) %&gt;%
    select(-c(value)) %&gt;%
    ## create a new variable that tells us whether the interval exlcudes zero or not  
    mutate(less = ifelse(lower_ci &lt;0 &amp; upper_ci &lt;0,1,0), more = ifelse(lower_ci &gt;0 &amp; upper_ci &gt;0,1,0)) %&gt;%
    mutate(tot = less + more) %&gt;%
    ungroup() %&gt;%
    summarise(total_significant_comparisons = sum(tot)) %&gt;%
    dplyr::pull()</code></pre>
<pre><code>## [1] 0</code></pre>
<p>Now that we’ve opened the guts of how to do it, let’s package all of the above into a function that we can then iterate over multiple simualted datasets.</p>
<pre class="r"><code>multilevel_aov_sim &lt;- function(seed) {
  set.seed(seed)
  d_bayes &lt;-
    tibble(treatmentA = rep(c(1, 2, 3, 4, 5), each = 3),
           response = rnorm(15, 0, 2))
  
  ## before running the function, we&#39;ll initialize a base model and update it - it&#39;s faster because it avoids recompiling the model in stan.
  
  mod_brms &lt;- update(mod_brms, newdata = d_bayes)
  
  ## extract estimates
  post &lt;- brms::posterior_samples(mod_brms) %&gt;%
    select(contains(&quot;r_treatmentA&quot;))
  
  ## make all the comparison between pairs   
  pairs &lt;- outer(colnames(post), colnames(post), paste, sep = &quot;-&quot;)
  index &lt;-  which(lower.tri(pairs, diag = TRUE))
  comparisons &lt;- outer(1:ncol(post), 1:ncol(post),
                       function(x, y)
                         post[, x] - post[, y])
  colnames(comparisons) &lt;- pairs
  comparisons &lt;- comparisons[-index]
  
  ## this gets us the number of comparisons that excluded zero
  comparisons %&gt;%
    gather() %&gt;%
    group_by(key) %&gt;%
    mutate(
      mean = mean(value),
      lower_ci = quantile(value, 0.025),
      upper_ci = quantile(value, 0.975)
    ) %&gt;%
    distinct(mean, .keep_all = TRUE) %&gt;%
    select(-c(value)) %&gt;%
    mutate(
      less = ifelse(lower_ci &lt; 0 &amp;
                      upper_ci &lt; 0, 1, 0),
      more = ifelse(lower_ci &gt; 0 &amp; upper_ci &gt; 0, 1, 0)
    ) %&gt;%
    mutate(tot = less + more) %&gt;%
    ungroup() %&gt;%
    summarise(total_significant_comparisons = sum(tot)) %&gt;%
    pull()
} </code></pre>
<pre class="r"><code>## we use the model mod_brms we initialized before  
seeds &lt;- sample(1:10000,1000, replace = FALSE)
t1 &lt;- Sys.time()
out &lt;- purrr::map(seeds, ~multilevel_aov_sim(seed = .x))
t2 &lt;- Sys.time()
t2-t1
saveRDS(out, &quot;/Users/dilsherdhillon/Documents/DilsherDhillon/static/files/out.rds&quot;)</code></pre>
<p>This took about ~ 18 minutes to run on my fairly recent macbook.</p>
<pre class="r"><code>multilevel_sim_results &lt;- readRDS(&quot;/Users/dilsherdhillon/Documents/DilsherDhillon/static/files/out.rds&quot;)</code></pre>
<p><strong>Results</strong></p>
<p>How many total pairwise comparisons excluded 0 in their 95% uncertainity intervals?</p>
<pre class="r"><code>multilevel_sim_results %&gt;% purrr::simplify() -&gt; multilevel_sim_results
s &lt;- sum(multilevel_sim_results)
s</code></pre>
<pre><code>## [1] 26</code></pre>
<p>That’s an interesting result. 26 intervals out of 10000 intervals excluded zero, while using weakly informative priors and no correction.</p>
<p>How many times did we see atleast one pairwise comparison significant?</p>
<pre class="r"><code>t &lt;- (1000 - sum(multilevel_sim_results==0))
t</code></pre>
<pre><code>## [1] 10</code></pre>
<p>Only 10 models run had atleast one pairwise comparison exclude zero - that’s an error rate of 0.01</p>
<p>This is without any correction, without controlling for an false positive rate whatsover. It’s a result of partial pooling or “shrinkage”, where the model learns information about the group to inform it’s own estimate.</p>
<p>For a visual representation, below is a comparison of the number of significant comparisons vs the model. For eg, the proportion of times atleast one pairwise comparison 95 % interval (incorrectly) excluded zero is ~12.5 % for a no correction Naive model, ~5 % of Tukey correction and &lt;1 % for a multilevel model.</p>
<pre class="r"><code>as.data.frame(cbind(rep(1:1000), naive_p,tukey_p,multilevel_sim_results)) %&gt;%
  rename(Multilevel_Model = multilevel_sim_results, Naive = naive_p, Tukey = tukey_p) %&gt;%
  gather(key = &quot;type&quot;, value = &quot;number_sig&quot;, -c(V1)) %&gt;%
  rename(seed = V1) %&gt;%
  group_by(type) %&gt;%
  count(number_sig) %&gt;%
  ggplot(.,aes(number_sig, n/1000)) + geom_point(aes(color = type)) + 
  labs(title = &quot;&quot;,
       x = &quot;Number of pairwise comparisons intervals excluding zero&quot;,
       y = &quot;Proportion&quot;)</code></pre>
<p><img src="/post/multiple_comparison_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>The above simulation excercise demonstrates that estimating effects under a multi-level framework is associated with a much lower false positive error rate, even when compared to the correction used in a frequentist inference. Since multilevel models are skeptical of extreme data, are able to “learn” from other groups, we are more likely to obtain precise intervals as compared to the traditional ANOVAs.</p>
<p>I would love to hear your thoughts on this and any comments are welcomed!</p>
<p>Thank you for reading!</p>
</div>
</div>
